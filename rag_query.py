#!/usr/bin/env python3
"""
RAG —Å–∏—Å—Ç–µ–º–∞: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python rag_query.py --query "How to configure BGP?" --model openai --api-key YOUR_KEY
    python rag_query.py --query "How to configure BGP?" --model gemini --api-key YOUR_GEMINI_KEY
"""

import os
import argparse
import chromadb
from sentence_transformers import SentenceTransformer
from chromadb.config import Settings

def expand_query_for_commands(query_text: str) -> str:
    """
    –†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–∞–Ω–¥.
    
    –ü—Ä–∏–º–µ—Ä—ã:
    - "ip address" -> "configure ip address on interface zebra command set"
    - "bgp neighbor" -> "configure bgp neighbor command setup"
    """
    query_lower = query_text.lower()
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
    tech_terms = {
        'ip address': 'configure ip address on interface zebra command set',
        'ipv6 address': 'configure ipv6 address on interface zebra command set',
        'bgp': 'border gateway protocol bgp configuration command',
        'ospf': 'open shortest path first ospf configuration command',
        'interface': 'network interface configuration zebra command',
        'neighbor': 'bgp neighbor peer configuration command',
        'route': 'routing table route configuration command',
        'static route': 'static routing configuration command',
    }
    
    expanded = query_text
    for term, expansion in tech_terms.items():
        if term in query_lower:
            expanded = f"{expanded} {expansion}"
            break  # –†–∞—Å—à–∏—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ—Ä–º–∏–Ω
    
    return expanded

def search_in_db(db_path: str, collection_name: str, query_text: str, 
                min_relevance: float = 0.1,
                max_results: int = 20,
                model_name: str = "all-mpnet-base-v2",
                auto_adjust: bool = True,
                use_hybrid: bool = True):
    """
    –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π + –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
    
    Args:
        db_path: –ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        min_relevance: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0.0-1.0). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –º–µ–Ω—å—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è.
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è embeddings
        auto_adjust: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–∞—Ç—å –ø–æ—Ä–æ–≥, –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç
        use_hybrid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
    """
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    client = chromadb.PersistentClient(path=db_path, settings=Settings(anonymized_telemetry=False))
    
    try:
        collection = client.get_collection(name=collection_name)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print(f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —Å–æ–∑–¥–∞–Ω–∞: python vectorize.py")
        return None, min_relevance
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è embeddings
    model = SentenceTransformer(model_name)
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥
    expanded_query = expand_query_for_commands(query_text)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å)
    query_embedding = model.encode([expanded_query], show_progress_bar=False).tolist()[0]
    
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    search_n_results = max_results * 2 if use_hybrid else max_results
    
    # –ü–æ–∏—Å–∫ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=search_n_results
    )
    
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    if use_hybrid and results and 'documents' in results and len(results['documents'][0]) > 0:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
        keywords = [word.lower() for word in query_text.split() if len(word) > 2]
        
        # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —á–µ—Ä–µ–∑ where_document
        semantic_ids = set(results['ids'][0] if 'ids' in results and results['ids'] else [])
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –∏—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –µ–≥–æ
        keyword_docs = []
        keyword_metas = []
        keyword_ids = []
        keyword_distances = []
        
        for keyword in keywords[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            try:
                # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                keyword_results = collection.query(
                    query_embeddings=[query_embedding],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ embedding
                    n_results=min(5, max_results),
                    where_document={"$contains": keyword}  # –§–∏–ª—å—Ç—Ä –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                )
                
                if keyword_results and 'ids' in keyword_results:
                    for i, kid in enumerate(keyword_results['ids'][0]):
                        if kid not in semantic_ids and kid not in keyword_ids:
                            keyword_ids.append(kid)
                            if 'documents' in keyword_results:
                                keyword_docs.append(keyword_results['documents'][0][i])
                            if 'metadatas' in keyword_results:
                                keyword_metas.append(keyword_results['metadatas'][0][i])
                            if 'distances' in keyword_results:
                                # –î–ª—è keyword –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                                dist = keyword_results['distances'][0][i] if i < len(keyword_results['distances'][0]) else 0.6
                                keyword_distances.append(dist)
            except Exception:
                # –ï—Å–ª–∏ where_document –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if keyword_docs:
            results['documents'][0].extend(keyword_docs)
            results['metadatas'][0].extend(keyword_metas)
            results['ids'][0].extend(keyword_ids)
            results['distances'][0].extend(keyword_distances)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    actual_threshold = min_relevance
    
    if 'distances' in results and results['distances'] and len(results['distances'][0]) > 0:
        distances = results['distances'][0]
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        ids = results['ids'][0]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_docs = []
        filtered_metas = []
        filtered_ids = []
        filtered_distances = []
        
        for i, distance in enumerate(distances):
            relevance = max(0, 1 - distance)  # –ó–∞—â–∏—Ç–∞ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            if relevance >= min_relevance:
                filtered_docs.append(documents[i])
                filtered_metas.append(metadatas[i])
                filtered_ids.append(ids[i])
                filtered_distances.append(distance)
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏ –≤–∫–ª—é—á–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        if len(filtered_docs) == 0 and auto_adjust and min_relevance > 0.05:
            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            best_relevance = max([max(0, 1 - d) for d in distances]) if distances else 0
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é > 0.05, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            if best_relevance >= 0.05:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ —á—É—Ç—å –Ω–∏–∂–µ –ª—É—á—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, –Ω–æ –Ω–µ –Ω–∏–∂–µ 0.05
                actual_threshold = max(0.05, best_relevance - 0.05)
                
                # –ü–µ—Ä–µ—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ–º —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º
                filtered_docs = []
                filtered_metas = []
                filtered_ids = []
                filtered_distances = []
                
                for i, distance in enumerate(distances):
                    relevance = max(0, 1 - distance)
                    if relevance >= actual_threshold:
                        filtered_docs.append(documents[i])
                        filtered_metas.append(metadatas[i])
                        filtered_ids.append(ids[i])
                        filtered_distances.append(distance)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results['documents'] = [filtered_docs]
        results['metadatas'] = [filtered_metas]
        results['ids'] = [filtered_ids]
        results['distances'] = [filtered_distances]
        results['actual_threshold'] = actual_threshold  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥
    
    return results, actual_threshold

def format_context(results, show_relevance: bool = False):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM.
    
    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏–∑ ChromaDB
        show_relevance: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    """
    context_parts = []
    
    distances = results.get('distances', [[]])[0] if 'distances' in results else []
    
    for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
        title = metadata.get('title', 'Unknown')
        section = metadata.get('section', 'N/A')
        url = metadata.get('url', 'N/A')
        
        relevance_info = ""
        if show_relevance and i <= len(distances):
            relevance = max(0, 1 - distances[i-1])
            relevance_info = f" [–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.3f}]"
        
        context_parts.append(f"""
[–î–æ–∫—É–º–µ–Ω—Ç {i}]{relevance_info}
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–†–∞–∑–¥–µ–ª: {section}
URL: {url}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:
{doc}
""")
    
    return "\n".join(context_parts)

def generate_answer_openai(query: str, context: str, api_key: str, model: str = "gpt-4o-mini"):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ OpenAI API.
    """
    try:
        from openai import OpenAI
    except ImportError:
        print("‚ùå –û—à–∏–±–∫–∞: openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
        return None
    
    client = OpenAI(api_key=api_key)
    
    prompt = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ FRRouting. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
{context}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
- –û—Ç–≤–µ—Ç—å —á–µ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –∫–æ–º–∞–Ω–¥–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–∏–≤–µ–¥–∏ —Ç–æ—á–Ω—É—é –∫–æ–º–∞–Ω–¥—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –î–ª—è –∫–æ–º–∞–Ω–¥ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: **–ö–æ–º–∞–Ω–¥–∞:** `–∫–æ–º–∞–Ω–¥–∞`
- –ü—Ä–∏–≤–µ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (URL) –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ–∫–∞–∂–∏ —Ç–æ—á–Ω—É—é –∫–æ–º–∞–Ω–¥—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏!

–û—Ç–≤–µ—Ç:"""
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ FRRouting, –ø–æ–º–æ–≥–∞—é—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI API: {e}")
        return None

def generate_answer_gemini(query: str, context: str, api_key: str, model: str = "gemini-2.5-flash"):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Google Gemini API.
    
    Args:
        query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        api_key: Google Gemini API –∫–ª—é—á
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (gemini-2.5-flash, gemini-2.5-pro, gemini-1.5-flash, gemini-1.5-pro)
    """
    try:
        import google.generativeai as genai
    except ImportError:
        print("‚ùå –û—à–∏–±–∫–∞: google-generativeai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install google-generativeai")
        return None
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º API
    genai.configure(api_key=api_key)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        # –ú–æ–¥–µ–ª–∏ –ø—Ä–∏—Ö–æ–¥—è—Ç —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "models/", –Ω–æ GenerativeModel –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞
        available_short = [m.replace("models/", "") for m in available_models]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –µ—Å–ª–∏ –µ—Å—Ç—å)
        model_short = model.replace("models/", "")
        
        if model_short not in available_short:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º flash –º–æ–¥–µ–ª–∏ (–±—ã—Å—Ç—Ä–µ–µ)
            flash_models = [m for m in available_short if "flash" in m.lower()]
            pro_models = [m for m in available_short if "pro" in m.lower()]
            
            selected_model = None
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: flash –º–æ–¥–µ–ª–∏ (–±—ã—Å—Ç—Ä–µ–µ)
            if flash_models:
                # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º 2.5, –ø–æ—Ç–æ–º 1.5
                for preferred in ["gemini-2.5-flash", "gemini-1.5-flash"]:
                    for flash in flash_models:
                        if flash == preferred or flash.startswith(preferred):
                            selected_model = flash
                            break
                    if selected_model:
                        break
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—É—é, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é flash
                if not selected_model:
                    selected_model = flash_models[0]
            
            # –ï—Å–ª–∏ flash –Ω–µ—Ç, –±–µ—Ä–µ–º pro
            elif pro_models:
                for preferred in ["gemini-2.5-pro", "gemini-1.5-pro"]:
                    for pro in pro_models:
                        if pro == preferred or pro.startswith(preferred):
                            selected_model = pro
                            break
                    if selected_model:
                        break
                
                if not selected_model:
                    selected_model = pro_models[0]
            
            if selected_model:
                print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å '{model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è '{selected_model}'")
                model = selected_model
            else:
                print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å '{model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è")
                model = available_short[0] if available_short else model_short
        else:
            model = model_short
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é (—É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –µ—Å–ª–∏ –µ—Å—Ç—å)
        model = model.replace("models/", "")
    
    prompt = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ FRRouting. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
{context}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
- –û—Ç–≤–µ—Ç—å —á–µ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –∫–æ–º–∞–Ω–¥–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–∏–≤–µ–¥–∏ —Ç–æ—á–Ω—É—é –∫–æ–º–∞–Ω–¥—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –î–ª—è –∫–æ–º–∞–Ω–¥ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: **–ö–æ–º–∞–Ω–¥–∞:** `–∫–æ–º–∞–Ω–¥–∞`
- –ü—Ä–∏–≤–µ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (URL) –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ–∫–∞–∂–∏ —Ç–æ—á–Ω—É—é –∫–æ–º–∞–Ω–¥—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏!

–û—Ç–≤–µ—Ç:"""
    
    try:
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å (–Ω–∞–∑–≤–∞–Ω–∏–µ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)
        model_instance = genai.GenerativeModel(model)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        generation_config = {
            "temperature": 0.3,
            "max_output_tokens": 2000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        }
        response = model_instance.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if not response.candidates:
            return "–û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—Ç–≤–µ—Ç"
        
        candidate = response.candidates[0]
        
        # –ü–æ–ª—É—á–∞–µ–º finish_reason (–º–æ–∂–µ—Ç –±—ã—Ç—å enum –∏–ª–∏ —á–∏—Å–ª–æ)
        finish_reason = candidate.finish_reason
        finish_reason_str = str(finish_reason)
        finish_reason_name = finish_reason.name if hasattr(finish_reason, 'name') else finish_reason_str
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        try:
            answer_text = response.text.strip()
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ (–¥–∞–∂–µ –µ—Å–ª–∏ finish_reason –Ω–µ STOP)
            if answer_text:
                return answer_text
        except (ValueError, AttributeError) as e:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º finish_reason
            pass
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ finish_reason –∫–æ–≥–¥–∞ —Ç–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        if "SAFETY" in finish_reason_name.upper() or finish_reason == 3:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –æ—Ç–≤–µ—Ç –±—ã–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."
        elif "MAX_TOKENS" in finish_reason_name.upper() or finish_reason == 2:
            return "–û—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å max_output_tokens."
        elif "RECITATION" in finish_reason_name.upper() or finish_reason == 4:
            return "–û—Ç–≤–µ—Ç –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."
        else:
            return f"–û—à–∏–±–∫–∞: finish_reason={finish_reason_name} ({finish_reason}). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Gemini API: {e}")
        print(f"   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å: --llm-model gemini-2.5-flash")
        return None

def main():
    parser = argparse.ArgumentParser(description="RAG —Å–∏—Å—Ç–µ–º–∞: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞")
    parser.add_argument("--query", "-q", required=True, help="–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    parser.add_argument("--db", default="vector_db", help="–ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î")
    parser.add_argument("--collection", default="frr_docs", help="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
    parser.add_argument("--min-relevance", type=float, default=0.1,
                       help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (0.0-1.0). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1")
    parser.add_argument("--max-results", type=int, default=20,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 20")
    parser.add_argument("--embedding-model", default="all-mpnet-base-v2", help="–ú–æ–¥–µ–ª—å –¥–ª—è embeddings")
    parser.add_argument("--show-relevance", action="store_true",
                       help="–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    parser.add_argument("--no-auto-adjust", action="store_true",
                       help="–û—Ç–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é –ø–æ—Ä–æ–≥–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
    parser.add_argument("--no-hybrid", action="store_true",
                       help="–û—Ç–∫–ª—é—á–∏—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (—Ç–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π)")
    
    # LLM –≤—ã–±–æ—Ä
    parser.add_argument("--model", choices=["openai", "gemini", "local"], default="gemini", 
                       help="–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ (openai, gemini, local)")
    parser.add_argument("--api-key", help="API –∫–ª—é—á (OpenAI –∏–ª–∏ Gemini, –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)")
    parser.add_argument("--llm-model", default="gemini-2.5-flash", 
                       help="–ú–æ–¥–µ–ª—å LLM (–¥–ª—è OpenAI: gpt-4o-mini, gpt-4; –¥–ª—è Gemini: gemini-2.5-flash, gemini-2.5-pro)")
    parser.add_argument("--show-sources", action="store_true", 
                       help="–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (URL –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
    
    args = parser.parse_args()
    
    # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
    print(f"üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è: '{args.query}'")
    print(f"   –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {args.min_relevance}, –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {args.max_results}")
    
    results, actual_threshold = search_in_db(
        db_path=args.db,
        collection_name=args.collection,
        query_text=args.query,
        min_relevance=args.min_relevance,
        max_results=args.max_results,
        model_name=args.embedding_model,
        auto_adjust=not args.no_auto_adjust,
        use_hybrid=not args.no_hybrid
    )
    
    if not results or not results['documents'] or len(results['documents'][0]) == 0:
        print("‚ùå –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print(f"   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å --min-relevance (—Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {args.min_relevance})")
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, –µ—Å–ª–∏ –ø–æ—Ä–æ–≥ –±—ã–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω
    if actual_threshold < args.min_relevance:
        print(f"   ‚ö†Ô∏è  –ü–æ—Ä–æ–≥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–µ–Ω –¥–æ {actual_threshold:.3f} (–±—ã–ª–æ {args.min_relevance})")
    
    num_found = len(results['documents'][0])
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {num_found} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å >= {actual_threshold:.3f})")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if args.show_relevance and 'distances' in results and results['distances']:
        print("\nüìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:")
        for i, distance in enumerate(results['distances'][0][:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
            relevance = max(0, 1 - distance)
            title = results['metadatas'][0][i-1].get('title', 'Unknown')[:50]
            print(f"   {i}. {relevance:.3f} - {title}...")
        if num_found > 5:
            print(f"   ... –∏ –µ—â–µ {num_found - 5} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        print()
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context = format_context(results, show_relevance=args.show_relevance)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...\n")
    
    if args.model == "openai":
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
        api_key = args.api_key or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            print("‚ùå –û—à–∏–±–∫–∞: OpenAI API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω")
            print("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --api-key –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é OPENAI_API_KEY")
            return
        
        answer = generate_answer_openai(
            query=args.query,
            context=context,
            api_key=api_key,
            model=args.llm_model
        )
    elif args.model == "gemini":
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
        api_key = args.api_key or os.environ.get("GEMINI_API_KEY")
        if not api_key:
            print("‚ùå –û—à–∏–±–∫–∞: Gemini API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω")
            print("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --api-key –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é GEMINI_API_KEY")
            print("   –ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á: https://makersuite.google.com/app/apikey")
            return
        
        answer = generate_answer_gemini(
            query=args.query,
            context=context,
            api_key=api_key,
            model=args.llm_model
        )
    elif args.model == "local":
        print("–õ–æ–∫–∞–ª—å–Ω—ã–µ LLM –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã.")
        return
    
    if answer:
        print("=" * 80)
        print("üìù –û–¢–í–ï–¢:")
        print("=" * 80)
        print(answer)
        print("=" * 80)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if args.show_sources:
            print("\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
            for i, metadata in enumerate(results['metadatas'][0], 1):
                url = metadata.get('url', 'N/A')
                title = metadata.get('title', 'Unknown')
                print(f"  {i}. {title}")
                print(f"     {url}")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç")

if __name__ == "__main__":
    main()

